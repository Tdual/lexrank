{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper: LexRank: Graph-based Lexical Centrality as Salience in Text Summarization\n",
    "https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import platform\n",
    "import MeCab\n",
    "import numpy as np\n",
    "import re\n",
    "from  urllib  import request\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.csgraph import connected_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "from urlextract import URLExtract\n",
    "\n",
    "EMAIL_REGEX = regex.compile(\n",
    "    r'[\\p{L}0-9]+[\\p{L}0-9_.+-]*[\\p{L}0-9_+-]+@[\\p{L}0-9]+[\\p{L}0-9.-]*\\.\\p{L}+'  # noqa\n",
    ")\n",
    "PUNCTUATION_SIGNS = set('.,;:¡!¿?…⋯&‹›«»\\\"“”[]()⟨⟩}{/|\\\\')\n",
    "\n",
    "url_extractor = URLExtract()\n",
    "\n",
    "\n",
    "\n",
    "class Tokenizer_en:\n",
    "    def __init__(self, stopwords):\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def clean_text(self, text, allowed_chars='- '):\n",
    "        text = ' '.join(text.lower().split())\n",
    "        text = ''.join(ch for ch in text if ch.isalnum() or ch in allowed_chars)\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def contains_letters(self, word):\n",
    "        return any(ch.isalpha() for ch in word)\n",
    "\n",
    "\n",
    "    def contains_numbers(self, word):\n",
    "        return any(ch.isdigit() for ch in word)\n",
    "\n",
    "\n",
    "    def filter_words(self, words, stopwords, keep_numbers=False):\n",
    "        if keep_numbers:\n",
    "            words = [\n",
    "                word for word in words\n",
    "                if (self.contains_letters(word) or self.contains_numbers(word)) and\n",
    "                word not in stopwords\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            words = [\n",
    "                word for word in words\n",
    "                if self.contains_letters(word) and not self.contains_numbers(word) and\n",
    "                word not in stopwords\n",
    "            ]\n",
    "\n",
    "        return words\n",
    "\n",
    "\n",
    "    def separate_punctuation(self, text):\n",
    "        text_punctuation = set(text) & PUNCTUATION_SIGNS\n",
    "\n",
    "        for ch in text_punctuation:\n",
    "            text = text.replace(ch, ' ' + ch + ' ')\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def tokenize(self, text, keep_numbers=False, keep_emails=False, keep_urls=False):\n",
    "        tokens = []\n",
    "\n",
    "        for word in text.split():\n",
    "            emails = EMAIL_REGEX.findall(word)\n",
    "\n",
    "            if emails:\n",
    "                if keep_emails:\n",
    "                    tokens.append(emails[0])\n",
    "\n",
    "                continue\n",
    "\n",
    "            urls = url_extractor.find_urls(word, only_unique=True)\n",
    "\n",
    "            if urls:\n",
    "                if keep_urls:\n",
    "                    tokens.append(urls[0].lower())\n",
    "\n",
    "                continue\n",
    "\n",
    "            cleaned = self.clean_text(self.separate_punctuation(word)).split()\n",
    "            cleaned = self.filter_words(cleaned, self.stopwords, keep_numbers=keep_numbers)\n",
    "\n",
    "            tokens.extend(cleaned)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer_en(stopwords=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr',\n",
       " 'osborne',\n",
       " 'said',\n",
       " 'the',\n",
       " 'coalition',\n",
       " 'government',\n",
       " 'was',\n",
       " 'planning',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'tax',\n",
       " 'system']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokenize(\"Mr Osborne said the coalition government was planning to change the tax system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer_ja:\n",
    "    def __init__(self, stopwords=None, parser=None, include_pos=None, exclude_posdetail=None, exclude_reg=None):\n",
    "        print(\"build Tokenizer\")\n",
    "        if stopwords:\n",
    "            self.stopwords = stopwords\n",
    "        else:\n",
    "            self.stopwords = self.get_stopwords()\n",
    "        self.include_pos = include_pos if include_pos else  [\"名詞\", \"動詞\", \"形容詞\"]\n",
    "        self.exclude_posdetail = exclude_posdetail if exclude_posdetail else [\"数\"]\n",
    "        self.exclude_reg = exclude_reg if exclude_reg else r\"$^\"  # no matching reg\n",
    "        if parser:\n",
    "            self.parser = parser\n",
    "        else:\n",
    "            self.parser = self.get_parser()\n",
    "            \n",
    "\n",
    "    def tokenize(self, text, show_pos=False):\n",
    "        text = re.sub(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", \"\", text)    #URL\n",
    "        text = re.sub(r\"\\\"?([-a-zA-Z0-9.`?{}]+\\.jp)\\\"?\" ,\"\", text)  # xxx.jp \n",
    "        text = text.lower()\n",
    "        l = [line.split(\"\\t\") for line in self.parser(text).split(\"\\n\")]\n",
    "        res = [\n",
    "            i[2] if not show_pos else (i[2],i[3]) for i in l \n",
    "                if len(i) >=4 # has POS.\n",
    "                    and i[3].split(\"-\")[0] in self.include_pos\n",
    "                    and i[3].split(\"-\")[1] not in self.exclude_posdetail\n",
    "                    and not re.search(r\"(-|−)\\d\", i[2])\n",
    "                    and not re.search(self.exclude_reg, i[2])\n",
    "                    #and i[2] not in self.stopwords          \n",
    "            ]\n",
    "        return res\n",
    "    \n",
    "    def get_parser(self):\n",
    "        if platform.system() == \"Darwin\":\n",
    "            dic_dir = \"/usr/local/lib/mecab/dic/mecab-ipadic-neologd/\" #mac\n",
    "        else:\n",
    "            dic_dir = \"/usr/lib/mecab/dic/mecab-ipadic-neologd\"\n",
    "        mecab = MeCab.Tagger(\"-Ochasen\")# -d {}\".format(dic_dir))\n",
    "        return mecab.parse\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_stopwords(self):\n",
    "        print(\"get stopwords from the web site.\")\n",
    "        res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\")\n",
    "        stopwords = [line.decode(\"utf-8\").strip() for line in res]\n",
    "        print(\"Japanese stopword: \", \", \".join(stopwords[:3]), \"...\")\n",
    "        res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/English.txt\")\n",
    "        stopwords += [line.decode(\"utf-8\").strip() for line in res]\n",
    "        print(\"English stopword: ...\", \", \".join(stopwords[-3:]), )\n",
    "        return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _power_method(transition_matrix, increase_power=True):\n",
    "    \n",
    "    eigenvector = np.ones(len(transition_matrix))\n",
    "\n",
    "    if len(eigenvector) == 1:\n",
    "        return eigenvector\n",
    "\n",
    "    transition = transition_matrix.transpose()\n",
    "\n",
    "    while True:\n",
    "        eigenvector_next = np.dot(transition, eigenvector)\n",
    "\n",
    "        if np.allclose(eigenvector_next, eigenvector):\n",
    "            return eigenvector_next\n",
    "\n",
    "        eigenvector = eigenvector_next\n",
    "\n",
    "        if increase_power:\n",
    "            transition = np.dot(transition, transition)\n",
    "\n",
    "\n",
    "def connected_nodes(matrix):\n",
    "    _, labels = connected_components(matrix)\n",
    "\n",
    "    groups = []\n",
    "\n",
    "    for tag in np.unique(labels):\n",
    "        group = np.where(labels == tag)[0]\n",
    "        groups.append(group)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def stationary_distribution(transition_matrix, increase_power=True, normalized=True):\n",
    "    size = len(transition_matrix)\n",
    "    distribution = np.zeros(size)\n",
    "\n",
    "    grouped_indices = connected_nodes(transition_matrix)\n",
    "\n",
    "    for group in grouped_indices:\n",
    "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
    "        eigenvector = _power_method(t_matrix, increase_power=increase_power)\n",
    "        distribution[group] = eigenvector\n",
    "\n",
    "    if normalized:\n",
    "        distribution /= size\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexRank:\n",
    "    def __init__(self, documents, tokenizer, include_new_words=True):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.include_new_words = include_new_words\n",
    "        self.idf_score = self._calculate_idf(documents)\n",
    "        \n",
    "\n",
    "    def get_summary(self, sentences, summary_size=1, threshold=.03, fast_power_method=True):\n",
    "        if not isinstance(summary_size, int) or summary_size < 1:\n",
    "            raise ValueError('\\'summary_size\\' should be a positive integer')\n",
    "            \n",
    "        lex_scores = self.rank_sentences(sentences, threshold=threshold,fast_power_method=fast_power_method) \n",
    "        sorted_ix = np.argsort(lex_scores)[::-1]\n",
    "        summary = [sentences[i] for i in sorted_ix[:summary_size]]\n",
    "        return summary\n",
    "\n",
    "    def rank_sentences(self, sentences, threshold=.03, fast_power_method=True):\n",
    "        if not (threshold is None or isinstance(threshold, float) and 0 <= threshold < 1):\n",
    "            raise ValueError(\" 'threshold' should be a floating-point number from the interval [0, 1) or None\")\n",
    "            \n",
    "        tf_scores = [Counter(self.tokenize_sentence(sentence)) for sentence in sentences]\n",
    "        similarity_matrix = self._calculate_similarity_matrix(tf_scores)\n",
    "        \n",
    "        if threshold is None:\n",
    "            markov_matrix = self._markov_matrix(similarity_matrix)\n",
    "        else:\n",
    "            markov_matrix = self._markov_matrix_discrete(similarity_matrix, threshold=threshold)\n",
    "        scores = stationary_distribution(markov_matrix, increase_power=fast_power_method, normalized=False)\n",
    "        return scores\n",
    "\n",
    "    def sentences_similarity(self, sentence_1, sentence_2):\n",
    "        tf_1 = Counter(self.tokenize_sentence(sentence_1))\n",
    "        tf_2 = Counter(self.tokenize_sentence(sentence_2))\n",
    "        similarity = self.get_simiarity([tf_1, tf_2], 0, 1)\n",
    "        return similarity\n",
    "\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        return tokens\n",
    "\n",
    "    def _calculate_idf(self, documents):\n",
    "        print(\"calculating idf\")\n",
    "        docs = [\" \".join(sentences) for sentences in tqdm(documents)]\n",
    "        tf = TfidfVectorizer(use_idf=True)\n",
    "        self.tf_matrix = tf.fit_transform(docs)\n",
    "        idf = tf.idf_ \n",
    "        if self.include_new_words:\n",
    "            default_value = np.log(len(docs) + 1)\n",
    "        else:\n",
    "            default_value = 0\n",
    "        idf_score = defaultdict(lambda: default_value)\n",
    "        for k, v in tf.vocabulary_.items():\n",
    "            idf_score[k] = idf[v] - 1        # idf = log(D/df) in the paper. no adding1.\n",
    "        return idf_score\n",
    "            \n",
    "    def _calculate_similarity_matrix(self, tf_scores):\n",
    "        length = len(tf_scores)\n",
    "        similarity_matrix = np.zeros([length] * 2)\n",
    "        for i in range(length):\n",
    "            for j in range(i, length):\n",
    "                similarity = self.get_simiarity(tf_scores, i, j)\n",
    "                if similarity:\n",
    "                    similarity_matrix[i, j] = similarity\n",
    "                    similarity_matrix[j, i] = similarity\n",
    "        return similarity_matrix\n",
    "    \n",
    "    def get_simiarity(self, tf_scores, i, j):\n",
    "        return self._idf_modified_cosine(tf_scores, i, j)\n",
    "\n",
    "    def _idf_modified_cosine(self, tf_scores, i, j):\n",
    "        if i == j:\n",
    "            return 1\n",
    "        tf_i, tf_j = tf_scores[i], tf_scores[j]\n",
    "        words_i, words_j = set(tf_i.keys()), set(tf_j.keys())\n",
    "        nominator = sum([tf_i[word] * tf_j[word] * self.idf_score[word] ** 2 for word in words_i & words_j])\n",
    "        if np.isclose(nominator, 0):\n",
    "            return 0\n",
    "        denominator_i = sum([(tf_i[word] * self.idf_score[word]) **2 for word in words_i])\n",
    "        denominator_j = sum([(tf_j[word] * self.idf_score[word]) **2 for word in words_j])\n",
    "        similarity = nominator / np.sqrt(denominator_i * denominator_j)\n",
    "        return similarity\n",
    "\n",
    "    def _markov_matrix(self, similarity_matrix):\n",
    "        row_sum = similarity_matrix.sum(axis=1, keepdims=True)\n",
    "        return similarity_matrix / row_sum\n",
    "\n",
    "    def _markov_matrix_discrete(self, similarity_matrix, threshold):\n",
    "        markov_matrix = np.zeros(similarity_matrix.shape)\n",
    "        for i in range(len(similarity_matrix)):\n",
    "            columns = np.where(similarity_matrix[i] > threshold)[0]\n",
    "            markov_matrix[i, columns] = 1 / len(columns)\n",
    "        return markov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4233b819a454336a3619d2a46732a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "documents_dir = Path('../Datasets/livedoor/ライブドアニュース/texts')\n",
    "\n",
    "for file_path in tqdm(documents_dir.glob('*.txt')):\n",
    "    with file_path.open(mode='rt', encoding='utf-8') as fp:\n",
    "        documents.append(fp.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build Tokenizer\n",
      "get stopwords from the web site.\n",
      "Japanese stopword:  あそこ, あたり, あちら ...\n",
      "English stopword: ... you've, z, zero\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer_ja()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['認める', 'もの', '自分', '自身', '若い', 'さ', '故', '過ち', 'もの']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokenize(\"認めたくないものだな。自分自身の若さ故の過ちというものを。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating idf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e787384f604838953c30b088b47a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7367), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lxr = LexRank(documents,tokenizer=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#../Datasets/livedoor/ライブドアニュース/texts/dokujo-tsushin-4778030.txt\n",
    "article = \"\"\"\n",
    "    http://news.livedoor.com/article/detail/4778030/\n",
    "2010-05-22T14:30:00+0900\n",
    "友人代表のスピーチ、独女はどうこなしている？\n",
    "　もうすぐジューン・ブライドと呼ばれる６月。独女の中には自分の式はまだなのに呼ばれてばかり……という「お祝い貧乏」状態の人も多いのではないだろうか？　さらに出席回数を重ねていくと、こんなお願いごとをされることも少なくない。\n",
    "\n",
    "　「お願いがあるんだけど……友人代表のスピーチ、やってくれないかな？」\n",
    "\n",
    "　さてそんなとき、独女はどう対応したらいいか？\n",
    "\n",
    "　最近だとインターネット等で検索すれば友人代表スピーチ用の例文サイトがたくさん出てくるので、それらを参考にすれば、無難なものは誰でも作成できる。しかし由利さん（33歳）はネットを参考にして作成したものの「これで本当にいいのか不安でした。一人暮らしなので聞かせて感想をいってくれる人もいないし、かといって他の友人にわざわざ聞かせるのもどうかと思うし……」ということで活用したのが、なんとインターネットの悩み相談サイトに。そこに作成したスピーチ文を掲載し「これで大丈夫か添削してください」とメッセージを送ったというのである。\n",
    "\n",
    "　「一晩で3人位の人が添削してくれましたよ。ちなみに自分以外にもそういう人はたくさんいて、その相談サイトには同じように添削をお願いする投稿がいっぱいありました」（由利さん）。ためしに教えてもらったそのサイトをみてみると、確かに「結婚式のスピーチの添削お願いします」という投稿が1000件を超えるくらいあった。めでたい結婚式の影でこんなネットコミュニティがあったとは知らなかった。\n",
    "\n",
    "　しかし「事前にお願いされるスピーチなら準備ができるしまだいいですよ。一番嫌なのは何といってもサプライズスピーチ！」と語るのは昨年だけで10万以上お祝いにかかったというお祝い貧乏独女の薫さん（35歳）\n",
    "\n",
    "　「私は基本的に人前で話すのが苦手なんですよ。だからいきなり指名されるとしどろもどろになって何もいえなくなる。そうすると自己嫌悪に陥って終わった後でもまったく楽しめなくなりますね」\n",
    "　\n",
    "　サプライズスピーチのメリットとしては、準備していない状態なので、フランクな本音をしゃべってもらえるという楽しさがあるようだ。しかしそれも上手に対応できる人ならいいが、苦手な人の場合だと「フランク」ではなく「しどろもどろ」になる危険性大。ちなみにプロの司会者の場合、本当のサプライズではなく式の最中に「のちほどサプライズスピーチとしてご指名させていただきます」という一言があることも多いようだが、薫さん曰く「そんな何分前に言われても無理！」らしい。要は「サプライズを楽しめる」というタイプの人選が大切ということか。\n",
    "\n",
    "　一方「ありきたりじゃつまらないし、ネットで例文を検索している際に『こんな方法もあるのか！』って思って取り入れました」という幸恵さん（30歳）が行ったスピーチは「手紙形式のスピーチ」というもの。\n",
    "\n",
    "　「○○ちゃんへ　みたいな感じで新婦の友人にお手紙を書いて読み上げるやり方です。これなら多少フランクな書き方でも大丈夫だし、何より暗記しないで堂々と読み上げることができますよね。読んだものはそのまま友人にあげれば一応記念にもなります」（幸恵さん）\n",
    "なるほど、確かにこれなら読みあげればいいだけなので、人前で話すのが苦手な人でも失敗しないかもしれない。\n",
    "\n",
    "　主役はあくまで新郎新婦ながらも、いざとなると緊張し、内容もあれこれ考えて、こっそりリハーサル……そんな人知れず頑張るスピーチ担当独女たちにも幸あれ（高山惠）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = article.replace(\"\\n\",\"\").replace(\" \",\"\").replace('\\u3000', '').split(\"。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://news.livedoor.com/article/detail/4778030/2010-05-22T14:30:00+0900友人代表のスピーチ、独女はどうこなしている？もうすぐジューン・ブライドと呼ばれる６月',\n",
       " '独女の中には自分の式はまだなのに呼ばれてばかり……という「お祝い貧乏」状態の人も多いのではないだろうか？さらに出席回数を重ねていくと、こんなお願いごとをされることも少なくない',\n",
       " '「お願いがあるんだけど……友人代表のスピーチ、やってくれないかな？」さてそんなとき、独女はどう対応したらいいか？最近だとインターネット等で検索すれば友人代表スピーチ用の例文サイトがたくさん出てくるので、それらを参考にすれば、無難なものは誰でも作成できる',\n",
       " 'しかし由利さん（33歳）はネットを参考にして作成したものの「これで本当にいいのか不安でした',\n",
       " '一人暮らしなので聞かせて感想をいってくれる人もいないし、かといって他の友人にわざわざ聞かせるのもどうかと思うし……」ということで活用したのが、なんとインターネットの悩み相談サイトに',\n",
       " 'そこに作成したスピーチ文を掲載し「これで大丈夫か添削してください」とメッセージを送ったというのである',\n",
       " '「一晩で3人位の人が添削してくれましたよ',\n",
       " 'ちなみに自分以外にもそういう人はたくさんいて、その相談サイトには同じように添削をお願いする投稿がいっぱいありました」（由利さん）',\n",
       " 'ためしに教えてもらったそのサイトをみてみると、確かに「結婚式のスピーチの添削お願いします」という投稿が1000件を超えるくらいあった',\n",
       " 'めでたい結婚式の影でこんなネットコミュニティがあったとは知らなかった',\n",
       " 'しかし「事前にお願いされるスピーチなら準備ができるしまだいいですよ',\n",
       " '一番嫌なのは何といってもサプライズスピーチ！」と語るのは昨年だけで10万以上お祝いにかかったというお祝い貧乏独女の薫さん（35歳）「私は基本的に人前で話すのが苦手なんですよ',\n",
       " 'だからいきなり指名されるとしどろもどろになって何もいえなくなる',\n",
       " 'そうすると自己嫌悪に陥って終わった後でもまったく楽しめなくなりますね」サプライズスピーチのメリットとしては、準備していない状態なので、フランクな本音をしゃべってもらえるという楽しさがあるようだ',\n",
       " 'しかしそれも上手に対応できる人ならいいが、苦手な人の場合だと「フランク」ではなく「しどろもどろ」になる危険性大',\n",
       " 'ちなみにプロの司会者の場合、本当のサプライズではなく式の最中に「のちほどサプライズスピーチとしてご指名させていただきます」という一言があることも多いようだが、薫さん曰く「そんな何分前に言われても無理！」らしい',\n",
       " '要は「サプライズを楽しめる」というタイプの人選が大切ということか',\n",
       " '一方「ありきたりじゃつまらないし、ネットで例文を検索している際に『こんな方法もあるのか！』って思って取り入れました」という幸恵さん（30歳）が行ったスピーチは「手紙形式のスピーチ」というもの',\n",
       " '「○○ちゃんへみたいな感じで新婦の友人にお手紙を書いて読み上げるやり方です',\n",
       " 'これなら多少フランクな書き方でも大丈夫だし、何より暗記しないで堂々と読み上げることができますよね',\n",
       " '読んだものはそのまま友人にあげれば一応記念にもなります」（幸恵さん）なるほど、確かにこれなら読みあげればいいだけなので、人前で話すのが苦手な人でも失敗しないかもしれない',\n",
       " '主役はあくまで新郎新婦ながらも、いざとなると緊張し、内容もあれこれ考えて、こっそりリハーサル……そんな人知れず頑張るスピーチ担当独女たちにも幸あれ（高山惠）']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77297297 1.18918919 1.24864865 1.12972973 1.18918919 1.07027027\n",
      " 1.01081081 1.12972973 1.07027027 0.47567568 1.12972973 0.89189189\n",
      " 1.12972973 1.18918919 0.89189189 1.18918919 0.35675676 1.18918919\n",
      " 0.41621622 1.12972973 1.12972973 1.07027027]\n"
     ]
    }
   ],
   "source": [
    "# get summary with classical LexRank algorithm\n",
    "summary = lxr.get_summary(sentences, summary_size=5)\n",
    "#print(summary)\n",
    "\n",
    "\n",
    "# get summary with continuous LexRank\n",
    "summary_cont = lxr.get_summary(sentences, threshold=None)\n",
    "#print(summary_cont)\n",
    "\n",
    "# get LexRank scores for sentences\n",
    "# 'fast_power_method' speeds up the calculation, but requires more RAM\n",
    "scores_cont = lxr.rank_sentences(\n",
    "    sentences,\n",
    "    #threshold=None,\n",
    "    #fast_power_method=False,\n",
    ")\n",
    "print(scores_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 15,  4,  1, 17, 13,  7, 10, 19,  3, 12, 20,  5,  8, 21,  6, 14,\n",
       "       11,  0,  9, 18, 16])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(scores_cont)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 7, 10, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_index = sorted(np.argsort(scores_cont)[::-1][:10])\n",
    "importance_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['独女の中には自分の式はまだなのに呼ばれてばかり……という「お祝い貧乏」状態の人も多いのではないだろうか？さらに出席回数を重ねていくと、こんなお願いごとをされることも少なくない',\n",
       " '「お願いがあるんだけど……友人代表のスピーチ、やってくれないかな？」さてそんなとき、独女はどう対応したらいいか？最近だとインターネット等で検索すれば友人代表スピーチ用の例文サイトがたくさん出てくるので、それらを参考にすれば、無難なものは誰でも作成できる',\n",
       " 'しかし由利さん（33歳）はネットを参考にして作成したものの「これで本当にいいのか不安でした',\n",
       " '一人暮らしなので聞かせて感想をいってくれる人もいないし、かといって他の友人にわざわざ聞かせるのもどうかと思うし……」ということで活用したのが、なんとインターネットの悩み相談サイトに',\n",
       " 'ちなみに自分以外にもそういう人はたくさんいて、その相談サイトには同じように添削をお願いする投稿がいっぱいありました」（由利さん）',\n",
       " 'しかし「事前にお願いされるスピーチなら準備ができるしまだいいですよ',\n",
       " 'そうすると自己嫌悪に陥って終わった後でもまったく楽しめなくなりますね」サプライズスピーチのメリットとしては、準備していない状態なので、フランクな本音をしゃべってもらえるという楽しさがあるようだ',\n",
       " 'ちなみにプロの司会者の場合、本当のサプライズではなく式の最中に「のちほどサプライズスピーチとしてご指名させていただきます」という一言があることも多いようだが、薫さん曰く「そんな何分前に言われても無理！」らしい',\n",
       " '一方「ありきたりじゃつまらないし、ネットで例文を検索している際に『こんな方法もあるのか！』って思って取り入れました」という幸恵さん（30歳）が行ったスピーチは「手紙形式のスピーチ」というもの',\n",
       " 'これなら多少フランクな書き方でも大丈夫だし、何より暗記しないで堂々と読み上げることができますよね']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for i,s in enumerate(sentences) if i in importance_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"en_stopwords\", \"r\") as f:\n",
    "    stopwords = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= stopwords.split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "documents_dir = Path(\"../Datasets/bbc/politics\")\n",
    "\n",
    "for file_path in documents_dir.glob('*.txt'):\n",
    "    with file_path.open(mode='rt', encoding='utf-8') as fp:\n",
    "        documents.append(fp.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer_en(stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating idf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b24c67bf45540ab9f0267b493e5d3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7367), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.9032258064516124, 0.9032258064516125, 1.1290322580645156, 0.9032258064516125, 0.6774193548387094, 1.3548387096774186, 1.1290322580645156]\n",
      "False\n",
      "['The BBC understands that as chancellor, Mr Osborne, along with the Treasury will retain responsibility for overseeing banks and financial regulation.', 'Mr Osborne said the coalition government was planning to change the tax system \"to make it fairer for people on low and middle incomes\", and undertake \"long-term structural reform\" of the banking sector, education and the welfare state.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lxr = LexRank(documents, tokenizer=t)\n",
    "\n",
    "sentences = [\n",
    "    'One of David Cameron\\'s closest friends and Conservative allies, '\n",
    "    'George Osborne rose rapidly after becoming MP for Tatton in 2001.',\n",
    "\n",
    "    'Michael Howard promoted him from shadow chief secretary to the '\n",
    "    'Treasury to shadow chancellor in May 2005, at the age of 34.',\n",
    "\n",
    "    'Mr Osborne took a key role in the election campaign and has been at '\n",
    "    'the forefront of the debate on how to deal with the recession and '\n",
    "    'the UK\\'s spending deficit.',\n",
    "\n",
    "    'Even before Mr Cameron became leader the two were being likened to '\n",
    "    'Labour\\'s Blair/Brown duo. The two have emulated them by becoming '\n",
    "    'prime minister and chancellor, but will want to avoid the spats.',\n",
    "\n",
    "    'Before entering Parliament, he was a special adviser in the '\n",
    "    'agriculture department when the Tories were in government and later '\n",
    "    'served as political secretary to William Hague.',\n",
    "\n",
    "    'The BBC understands that as chancellor, Mr Osborne, along with the '\n",
    "    'Treasury will retain responsibility for overseeing banks and '\n",
    "    'financial regulation.',\n",
    "\n",
    "    'Mr Osborne said the coalition government was planning to change the '\n",
    "    'tax system \\\"to make it fairer for people on low and middle '\n",
    "    'incomes\\\", and undertake \\\"long-term structural reform\\\" of the '\n",
    "    'banking sector, education and the welfare state.',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "scores_cont = lxr.rank_sentences(sentences)\n",
    "print(scores_cont.tolist())\n",
    "print(scores_cont.tolist() == [1.0526315789473673, 0.5263157894736842, 1.0526315789473673, 1.0, 1.0, 1.3157894736842095, 1.0526315789473673])\n",
    "\n",
    "\n",
    "# get summary with classical LexRank algorithm\n",
    "summary = lxr.get_summary(sentences, summary_size=2)\n",
    "print(summary)\n",
    "\n",
    "summary ==  [\n",
    "    'The BBC understands that as chancellor, Mr Osborne, along with the Treasury '\n",
    "    'will retain responsibility for overseeing banks and financial regulation.',\n",
    "    \n",
    "    'Mr Osborne said the coalition government was planning to change the tax '\n",
    "    'system \"to make it fairer for people on low and middle incomes\", and '\n",
    "    'undertake \"long-term structural reform\" of the banking sector, education and '\n",
    "    'the welfare state.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py3.6)",
   "language": "python",
   "name": "conda_py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
